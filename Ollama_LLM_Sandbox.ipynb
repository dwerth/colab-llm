{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpRFIluKrbud7dlFINJLE0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwerth/colab-llm/blob/main/Ollama_LLM_Sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ollama in Colab v0.2\n",
        "\n",
        "Notes: select the \"T4 GPU\" from Runtime > Change Runtime Type"
      ],
      "metadata": {
        "id": "4RSE2N_oksAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xkS9GAgoS-x",
        "outputId": "38669366-69bd-4fd1-e021-f78689014d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ollama: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only run this if \"!ollama --version\" returns an error, warnings are ok\n",
        "%%capture\n",
        "!apt-get update && apt-get install -y lshw\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "FOYWeATvvHB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama serve > /dev/null 2>&1 &\n",
        "!pip install ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjIn8Z-39Ocp",
        "outputId": "2629ff8c-eecf-443c-bf55-d86f4a6b6b62",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
            "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "_oBAyFnZphrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run the next cell and then select a model to use"
      ],
      "metadata": {
        "id": "ad6d_2x5-Ab6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Define a list of commonly used Ollama models\n",
        "available_models = [\n",
        "    \"deepseek-r1:1.5b\",\n",
        "    \"gemma3:1b\",\n",
        "    \"granite3.1-moe:1b\",\n",
        "    \"granite3.1-moe:3b\",\n",
        "    \"ollama llama3.2:1b\",\n",
        "    \"phi3\",\n",
        "    \"qwen2.5:0.5b\",\n",
        "    \"qwen2.5:1.5b\",\n",
        "    \"smollm2:1.7b\",\n",
        "    \"smollm2:135m\",\n",
        "    \"tinyllama\",\n",
        "    \"erwan2/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "]\n",
        "\n",
        "# Create a dropdown widget for model selection\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=available_models,\n",
        "    description='Select model:',\n",
        "    style={'description_width': 'initial'},\n",
        "    layout={'width': '300px'},\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Create a button to pull the selected model\n",
        "pull_button = widgets.Button(\n",
        "    description='Pull and Use Selected Model',\n",
        "    button_style='primary',\n",
        "    tooltip='Click to pull the selected model',\n",
        "    layout={'width': '200px', 'margin': '10px 0'}\n",
        ")\n",
        "\n",
        "# Create an output area for status messages\n",
        "output_area = widgets.Output(\n",
        "    layout={'border': '1px solid #ddd', 'width': '500px', 'min_height': '100px', 'max_height': '300px', 'overflow': 'auto', 'padding': '10px'}\n",
        ")\n",
        "\n",
        "# Define the button click handler\n",
        "def on_pull_button_click(b):\n",
        "    global model_name\n",
        "    model_name = model_dropdown.value\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        print(f\"Pulling model: {model_name}\")\n",
        "        print(\"This may take a while depending on the model size...\")\n",
        "        try:\n",
        "            # Execute the ollama pull command\n",
        "            !ollama pull {model_name}\n",
        "            print(f\"\\n‚úÖ Successfully pulled {model_name} and it's ready to use\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error pulling model: {e}\")\n",
        "\n",
        "# Connect the button click handler\n",
        "pull_button.on_click(on_pull_button_click)\n",
        "\n",
        "# Display the widgets\n",
        "print(\"ü§ñ Ollama Model Puller\")\n",
        "print(\"Select a model from the dropdown and click the button to pull and select it for use.\")\n",
        "display(model_dropdown)\n",
        "display(pull_button)\n",
        "display(output_area)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DPVsUxPIzSdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change to reflect the content of the tile you're interested in using\n",
        "tile_text = \"coffee\"\n",
        "tile_image = \"full cup of coffee\""
      ],
      "metadata": {
        "id": "FvgDqBbwymwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = ollama.chat(model=model_name, messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (f\"\"\"\n",
        "            You are a helpful assistant generating short phrases\n",
        "            I am person with aphasia using a speach generating device to communicate with another person\n",
        "            I've selected a {tile_text} tile on my device, what might I say to communicate my needs?\n",
        "            \"\"\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": (f\"\"\"\n",
        "            The {tile_text} tile has a picture of {tile_image}\n",
        "            I want to say something positive about my needs related to {tile_text}\n",
        "            Generate 3 simple phrases for me to select from.\n",
        "            \"\"\"\n",
        "        )\n",
        "    }\n",
        "])\n",
        "print(\"Model:\", model_name, \"\\n\")\n",
        "print(response['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpYaNBzrlkut",
        "outputId": "21aea123-c00b-4118-9259-9f5cc26f6c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: erwan2/DeepSeek-R1-Distill-Qwen-1.5B \n",
            "\n",
            "<think>\n",
            "Okay, so the user mentioned they have aphasia and are using a speech-generating device. They selected a coffee tile on their device, which shows a full cup of coffee in a picture.\n",
            "\n",
            "They want to communicate their needs with another person, so I need to think about positive phrases related to their coffee habits. The goal is simple and easy to understand since they're dealing with aphasia.\n",
            "\n",
            "First, maybe something like \"I enjoy my coffee\" or \"My cup has always had a sweet taste.\" Those are straightforward and convey satisfaction.\n",
            "\n",
            "Then, suggestions could include words like \"consistent,\" \"timeless,\" or \"reliable.\" These show commitment and quality over time, which is good for someone with aphasia who might need more structure in communication.\n",
            "\n",
            "I should make sure the phrases are clear and positive, avoiding any negative connotations. Also, using common expressions can help them convey their needs without it being hard to understand.\n",
            "\n",
            "Overall, aiming for something that sounds inviting and shows they're looking for support is key. These phrases should encourage others to reach out with their coffee preferences.\n",
            "</think>\n",
            "\n",
            "1. \"I enjoy my coffee.\"  \n",
            "2. \"My cup has always had a sweet taste.\"  \n",
            "3. \"Consistent with the style of my coffee.\"  \n",
            "4. \"My time as a customer seems timeless and reliable.\"  \n",
            "5. \"The flavor I enjoy in my coffee is consistently on point.\"  \n",
            "\n",
            "These phrases are straightforward, inviting, and positive, which might help others communicate their needs to you or someone else.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = ollama.chat(model=model_name, messages=[\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (f\"\"\"\n",
        "            You are a helpful assistant generating short phrases\n",
        "            I am person with aphasia using a speach generating device to communicate with another person\n",
        "            I've selected a {tile_text} tile on my device, what might I say to communicate my needs?\n",
        "            \"\"\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": (f\"\"\"\n",
        "            The {tile_text} tile has a picture of {tile_image}\n",
        "            I want to say something negative about my needs related to {tile_text}\n",
        "            Generate 3 simple phrases for me to select from.\n",
        "            \"\"\"\n",
        "        )\n",
        "    }\n",
        "])\n",
        "print(\"Model:\", model_name, \"\\n\")\n",
        "print(response['message']['content'])"
      ],
      "metadata": {
        "id": "8z0vu-GTwQex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fupu_SAxye8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}